{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choropleth mapping\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import seaborn\n",
    "import pandas\n",
    "import geopandas\n",
    "import pysal\n",
    "import numpy\n",
    "from choropleth import choropleth\n",
    "import mapclassify\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principles\n",
    "\n",
    "\n",
    "Choropleth maps play a prominent role in geographic data science as they allow us to display non-geographic attributes or variables on a geographic map. The word\n",
    "choropleth stems from the root \"choro\", meaning \"region\". As such choropleth maps\n",
    "represent data at the region level, and\n",
    "are appropriate for areal unit data where each observation combines a value of\n",
    "an attribute and a geometric figure, usually a polygon. Choropleth maps derive from an earlier era where\n",
    "cartographers faced technological constraints that precluded the use of\n",
    "unclassed maps where each unique attribute value could be represented by a\n",
    "distinct symbol or color. Instead, attribute values were grouped into a smaller number of\n",
    "classes, usually not more than 12. Each class was associated with a unique symbol that was in turn\n",
    "applied to all observations with attribute values falling in the class.\n",
    "\n",
    "Although today these technological constraints are no longer binding, and\n",
    "unclassed mapping is feasible, there are still good reasons for adopting a\n",
    "classed approach. Chief among these is to reduce the cognitive load involved in\n",
    "parsing the complexity of an unclassed map. A choropleth map reduces this\n",
    "complexity by drawing upon statistical and visualization theory to provide an\n",
    "effective representation of the spatial distribution of the attribute values\n",
    "across the areal units. \n",
    "\n",
    "The effectiveness of a choropleth map will be a\n",
    "function of the choice of classification scheme together with the color or\n",
    "symbolization strategy adopted. In broad terms, the classification scheme\n",
    "defines the number of classes as well as the rules for assignment, while the\n",
    "symbolization should convey information about the value differentiation across\n",
    "the classes.\n",
    "\n",
    "In this chapter we first discuss the approaches used to classify\n",
    "attribute values. This is followed by an overview of color theory and the\n",
    "implications of different color schemes for effective map design. We  combine\n",
    "theory and practice by exploring how these concepts are implemented in different Python packages, including `geopandas`, and `PySAL`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative data classification \n",
    "\n",
    "Data classification considers the problem of \n",
    "partitioning the attribute values into mutually exclusive and exhaustive\n",
    "groups. The precise manner in which this is done will be a function of the\n",
    "measurement scale of the attribute in question. For quantitative attributes\n",
    "(ordinal, interval, ratio scales) the classes will have an explicit ordering.\n",
    "More formally, the classification problem is to define class boundaries such\n",
    "that\n",
    "$$\n",
    "c_j < y_i \\le  c_{j+1} \\ \\forall y_i \\in C_{j+1}\n",
    "$$\n",
    "where $y_i$ is the\n",
    "value of the attribute for spatial location $i$, $j$ is a class index, and $c_j$\n",
    "represents the lower bound of interval $j$.\n",
    "\n",
    "Different classification schemes obtain from their definition of the class\n",
    "boundaries. The choice of the classification scheme should take into\n",
    "consideration the statistical distribution of the attribute values.\n",
    "\n",
    "To illustrate these considerations, we will examine regional income data for the\n",
    "32 Mexican states. The variable we focus on is per capita gross domestic product\n",
    "for 1940 (PCGDP1940):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx = geopandas.read_file(\"data/mexicojoin.shp\")\n",
    "mx[['NAME', 'PCGDP1940']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which displays the following statistical distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = seaborn.distplot(mx['PCGDP1940'], bins=5, rug=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the distribution is positively skewed as in common in regional income studies. In other words,\n",
    "the mean exceeds the median (`50%`, in the table below), leading the to fat right tail in the figure. As\n",
    "we shall see, this skewness will have implications for the choice of choropleth\n",
    "classification scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx['PCGDP1940'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification schemes\n",
    "\n",
    "For quantitative attributes we first sort the data by their value,\n",
    "such that $x_0 \\le x_2 \\ldots \\le x_{n-1}$. For a prespecified number of classes\n",
    "$k$, the classification problem boils down to selection of $k-1$ break points\n",
    "along the sorted values that separate the values into mutually exclusive and\n",
    "exhaustive groups.\n",
    "\n",
    "In fact, the determination of the histogram above can\n",
    "be viewed as one approach to this selection.\n",
    "The method `seaborn.distplot` uses the matplotlib `hist`\n",
    "function under the hood to determine the class boundaries and the counts of\n",
    "observations in each class. In the figure, we have five classes which can be\n",
    "extracted with an explicit call to the `hist` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, bins, patches = h.hist(mx['PCGDP1940'], bins=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `counts` object captures how many observations each category in the classification has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `bin` object stores these break points we are interested in when considering classification schemes (the `patches` object can be ignored in this context, as it stores the geometries of the histogram plot):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This yields 5 bins, with the first having a lower bound of 1892 and an upper\n",
    "bound of 5985.8 which contains 17 observations. \n",
    "The determination of the\n",
    "interval width ($w$) and the number of bins in `seaborn` is based on the Freedman-Diaconis rule:\n",
    "\n",
    "$$w = 2 * IQR * n^{-1/3}$$\n",
    "\n",
    "where $IQR$ is the inter quartile\n",
    "range of the attribute values. Given $w$ the number of bins ($k$) is:\n",
    "\n",
    "$$k=(max-\n",
    "min)/w.$$\n",
    "\n",
    "Below we present several approaches to create these break points that follow criteria that can be of interest in different contests, as they focus on different priorities.\n",
    " \n",
    "#### Equal Intervals\n",
    "\n",
    "The Freedman-Diaconis approach provides a rule to determine\n",
    "the width and, in turn, the number of bins for the classification. This is a\n",
    "special case of a more general classifier known as \"equal intervals\", where each\n",
    "of the bins has the same width in the value space. \n",
    "For a given value of $k$, equal intervals\n",
    "classification splits the range of the attribute space into equal lengthened\n",
    "intervals, with each interval having a width\n",
    "$w = \\frac{x_0 - x_{n-1}}{k-1}$.\n",
    "Thus the maximum class is $(x_{n-1}-w, x_{n-1}]$ and the first class is\n",
    "$(-\\infty, x_{n-1} - (k-1)w]$.\n",
    "\n",
    "Equal intervals have the dual advantages of\n",
    "simplicity and ease of interpretation. However, this rule only considers the extreme\n",
    "values of the distribution and, in some cases, this can result in one or more\n",
    "classes being sparse. This is clearly the case in our income dataset, as the majority of\n",
    "the values are placed into the first two classes leaving the last three classes\n",
    "rather sparse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ei5 = mapclassify.Equal_Interval(mx['PCGDP1940'], k=5)\n",
    "ei5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Note that each of the intervals, however, has equal width of\n",
    "$w=4093.8$. This value of $k=5$ also coincides with the default classification\n",
    "in the Seaborn histogram in Figure 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantiles\n",
    "To avoid the potential problem of sparse classes, the quantiles of\n",
    "the distribution can be used to identify the class boundaries. Indeed, each\n",
    "class will have approximately $\\mid\\frac{n}{k}\\mid$ observations using the quantile\n",
    "classifier. If $k=5$ the sample quintiles are used to define the upper limits of\n",
    "each class resulting in the following classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q5 = mapclassify.Quantiles(mx.PCGDP1940, k=5)\n",
    "q5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while the numbers of values in each class are roughly equal, the\n",
    "widths of the first four intervals are rather different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q5.bins[1:]-q5.bins[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While quantiles does avoid the pitfall of sparse classes, this classification is\n",
    "not problem free. The varying widths of the intervals can be markedly different\n",
    "which can lead to problems of interpretation. A second challenge facing quantiles\n",
    "arises when there are a large number of duplicate values in the distribution\n",
    "such that the limits for one or more classes become ambiguous.\n",
    "\n",
    "#### Mean-standard deviation\n",
    "\n",
    "Our third classifer uses the sample mean $\\bar{x} =\n",
    "\\frac{1}{n} \\sum_{i=1}^n x_i$ and sample standard deviation $s = \\sqrt{\n",
    "\\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})  }$ to define class boundaries as\n",
    "some distance from the sample mean, with the distance being a multiple of the\n",
    "standard deviation. For example, a common definition for $k=5$ is to set the\n",
    "upper limit of the first class to two standard deviations ($c_{0}^u = \\bar{x} - 2 s$), and the intermediate\n",
    "classes to have upper limits within one standard deviation ($c_{1}^u = \\bar{x}-s,\\ c_{2}^u = \\bar{x}+s, \\ c_{3}^u\n",
    "= \\bar{x}+2s$). Any values greater (smaller) than two standard deviations above (below) the mean\n",
    "are placed into the top (bottom) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msd = mapclassify.Std_Mean(mx['PCGDP1940'])\n",
    "msd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classifier is best used when data is normally distributed or, at least, when the sample mean is a meaningful measure to anchor the classification around. Clearly this is\n",
    "not the case for our income data as the positive skew results in a loss of\n",
    "information when we use the standard deviation. The lack of symmetry thus leads to\n",
    "inadmissible boundaries for the first  class as well as a concentration of the\n",
    "vast majority of values in the middle class.\n",
    "\n",
    "#### Maximum Breaks\n",
    "\n",
    "The maximum breaks classifier decides where to set the break points between classes by considering the difference between sorted values. That is, rather than considering a value of the dataset in itself, it looks at how appart each value is from the next one in the sorted sequence. The classifier then places the the $k-1$ break points in between the $k$ values most stretched apart from each other in the entire sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb5 = mapclassify.Maximum_Breaks(mx['PCGDP1940'], k=5)\n",
    "mb5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum breaks is an appropriate approach when we are interested in making sure observations in each class are as similar to each other as possible. As such, it works well in cases where the distribution of values is not unimodal. In addition, the algorithm is relatively fast to compute. However, its simplicitly can sometimes cause unexpected results. To the extent in only considers the top $k$ differences between consecutive values, other more nuanced within-group differences and dissimilarities can be ignored.\n",
    "\n",
    "#### Box-Plot\n",
    "\n",
    "The box-plot classification is a blend of the quantile and\n",
    "standard deviation classifiers. Here $k$ is predefined to six, with the upper limit of class 0 set\n",
    "to $q_{0.25}-h \\, IQR$. $IQR = q_{0.75}-q_{0.25}$ is the\n",
    "inter-quartile range; $h$ corresponds to the hinge, or the multiplier of the $IQR$ to obtain the bounds of the whiskers. The lower limit of the sixth class is set to $q_{0.75}+h \\,\n",
    "IQR$. Intermediate classes have their upper limits set to the 0.25, 0.50 and\n",
    "0.75 percentiles of the attribute values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp = mapclassify.Box_Plot(mx['PCGDP1940'])\n",
    "bp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any values falling into either of the extreme classes are defined as outlers.\n",
    "Note that because the income values are non-negative by definition, the lower\n",
    "outlier class has an inadmissible upper bound meaning that lower outliers would\n",
    "not be possible for this sample.\n",
    "\n",
    "The default value for the hinge is $h=1.5$ in\n",
    "PySAL. However, this can be specified by the user for an alternative classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp1 = mapclassify.Box_Plot(mx['PCGDP1940'], hinge=1)\n",
    "bp1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing so will affect the definition of the outlier classes, as well as the\n",
    "neighboring internal classes.\n",
    "\n",
    "#### Head Tail Breaks\n",
    "\n",
    "The head tail algorithm, introduced by Jiang (2013) is based on a recursive partioning of the data using splits around\n",
    "iterative means. The splitting process continues until the distributions within each of\n",
    "the classes no longer display a heavy-tailed distribution in the sense that\n",
    "there is a balance between the number of smaller and larger values assigned to\n",
    "each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht = mapclassify.HeadTail_Breaks(mx['PCGDP1940'])\n",
    "ht"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data with a heavy-tailed distribution, such as power law and log normal\n",
    "distributions, the head tail breaks classifier (Jiang 2015) can be particularly\n",
    "effective.\n",
    "\n",
    "#### Jenks Caspall\n",
    "\n",
    "This approach, as well as the following two, tackles the classification challenge from a heuristic perspective, rather than from deterministic one. Originally proposed by Jenks & Caspall (1971), this algorithm aims to minimize the sum of absolute deviations around\n",
    "class means. The approach begins with a prespecified number of classes and an\n",
    "arbitrary initial set of class breaks - for example using quintiles. The\n",
    "algorithm attempts to improve the objective function by considering the movement\n",
    "of observations between adjacent classes. For example, the largest value in the\n",
    "lowest quintile would be considered for movement into the second quintile, while\n",
    "the lowest value in the second quintile would be considered for a possible move\n",
    "into the first quintile. The candidate move resulting in the largest reduction\n",
    "in the objective function would be made, and the process continues until no\n",
    "other improving moves are possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.random.seed(12345)\n",
    "jc5 = mapclassify.Jenks_Caspall(mx['PCGDP1940'], k=5)\n",
    "jc5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fisher Jenks\n",
    "\n",
    "The second optimal algorithm adopts a dynamic programming approach to minimize\n",
    "the sum of the absolute deviations around class medians. In contrast to the\n",
    "Jenks-Caspall approach, Fisher-Jenks is guaranteed to produce an optimal\n",
    "classification for a prespecified number of classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.random.seed(12345)\n",
    "fj5 = mapclassify.Fisher_Jenks(mx['PCGDP1940'], k=5)\n",
    "fj5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max-p\n",
    "\n",
    "Finally, the max-p classifiers adopts the algorithm underlying the max-p region\n",
    "building method (Duque, Anselin and Rey, 2012) to the case of map classification. It is similar in spirit to\n",
    "Jenks Caspall in that it considers greedy swapping between adjacent classes to\n",
    "improve the objective function. It is a heuristic, however, so unlike\n",
    "Fisher-Jenks, there is no optimial solution guaranteed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp5 = mapclassify.Max_P_Classifier(mx['PCGDP1940'], k=5)\n",
    "mp5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Classification schemes\n",
    "\n",
    "As a special case of clustering, the definition of\n",
    "the number of classes and the class boundaries pose a problem to the map\n",
    "designer. Recall that the Freedman-Diaconis rule was said to be optimal,\n",
    "however, the optimality necessitates the specification of an objective function.\n",
    "In the case of Freedman-Diaconis the objective function is to minimize the\n",
    "difference between the area under estimated kernel density based on the sample\n",
    "and the area under the theoretical population distribution that generated the\n",
    "sample.\n",
    "\n",
    "This notion of statistical fit is an important one, however, not the\n",
    "only consideration when evaluating classifiers for the purpose of choropleth\n",
    "mapping. Also relevant is the spatial distribution of the attribute values and\n",
    "the ability of the classifier to convey a sense of that spatial distribution. As\n",
    "we shall see, this is not necessarily directly related to the statistical\n",
    "distribution of the attribute values. We will return to a joint consideration of both\n",
    "the statistical and spatial distribution of the attribute values in comparison\n",
    "of classifiers below.\n",
    "\n",
    "For map classification, one optimiality criterion that\n",
    "can be used is a measure of fit. In PySAL the \"absolute deviation around class\n",
    "medians\" (ADCM) is calculated and provides a measure of fit that allows for\n",
    "comparison of alternative classifiers for the same value of $k$.\n",
    "\n",
    "To see this, we can compare different classifiers for $k=5$ on the Mexico data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class5 = q5, ei5, ht, mb5, msd, fj5, jc5\n",
    "fits = numpy.array([ c.adcm for c in class5])\n",
    "data = pandas.DataFrame(fits)\n",
    "data['classifier'] = [c.name for c in class5]\n",
    "data.columns = ['ADCM', 'Classifier']\n",
    "ax = seaborn.barplot(y='Classifier', x='ADCM', data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is to be expected, the Fisher-Jenks classifier dominates all other k=5\n",
    "classifiers with an ACDM of 23,729. Interestingly, the equal interval classifier\n",
    "performs well despite the problems associated with being sensitive to the\n",
    "extreme values in the distribution. The mean-standard deviation classifier has a\n",
    "very poor fit due to the skewed nature of the data and the concentrated\n",
    "assignment of the majority of the observations to the central class.\n",
    "\n",
    "The ADCM provides a global measure of fit which can be used to compare the\n",
    "alternative classifiers. As a complement to this global perspective, it can be\n",
    "revealing to consider how each of the spatial observations was classified across\n",
    "the alternative approaches. To do this we can add the class bin attribute (`yb`)\n",
    "generated by the PySAL classifiers as additional columns in the data frame and\n",
    "present these jointly in a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx['q540'] = q5.yb\n",
    "mx['ei540'] = ei5.yb\n",
    "mx['ht40'] = ht.yb\n",
    "mx['mb540'] = mb5.yb\n",
    "mx['msd40'] = msd.yb\n",
    "mx['fj540'] = fj5.yb\n",
    "mx['jc540'] = jc5.yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mxs = mx.sort_values('PCGDP1940')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_values(val):\n",
    "    if val==0:\n",
    "        return 'background-color: %s' % '#ffffff'\n",
    "    elif val==1:\n",
    "        return 'background-color: %s' % '#e0ffff'\n",
    "    elif val==2:\n",
    "        return 'background-color: %s' % '#b3ffff'\n",
    "    elif val==3:\n",
    "        return 'background-color: %s' % '#87ffff'\n",
    "    elif val==4:\n",
    "        return 'background-color: %s' % '#62e4ff'\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = mxs[['NAME', 'PCGDP1940', 'q540', 'ei540', 'ht40', 'mb540', 'msd40', 'fj540', 'jc540']]\n",
    "t.style.applymap(highlight_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspection of this table reveals a number of interesting results. First, the\n",
    "only Mexican state that is treated consistantly across the k=5 classifiers is\n",
    "Baja California Norte which is placed in the highest class by all classifiers.\n",
    "Second, the mean-standard deviation classifier has an empty first class due to\n",
    "the inadmissible upper bound and the overconcentration of values in the central\n",
    "class (2).\n",
    "\n",
    "Finally, we can consider a meso-level view of the clasification\n",
    "results by comparing the number of values assigned to each class across the\n",
    "different classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.DataFrame({c.name: c.counts for c in class5},\n",
    "                 index=['Class-{}'.format(i) for i in range(5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing so highlights the similarities between Fisher Jenks and equal intervals as\n",
    "the distribution counts are very similar as the two approaches agree on all 17\n",
    "states assigned to the first class. Indeed, the only observation that\n",
    "distinguishes the two classifiers is the treatment of Baja California Sur which\n",
    "is kept in class 1 in equal intervals, but assigned to class 2 by Fisher Jenks.\n",
    "\n",
    "### Color\n",
    "\n",
    "Having considered the evaluation of the statisitcal distribution of\n",
    "the attribute values and the alternative classification approaches, the next\n",
    "step is to select the symbolization and color scheme. Together with the choice of classifier, these will determine the overall\n",
    "effectiveness of the choropleth map in representing the spatial\n",
    "distribution of the attribute values.\n",
    "\n",
    "Let us start by refreshing the `mx` object and exploring the base polygons for the Mexican states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx = geopandas.read_file(\"data/mexicojoin.shp\")\n",
    "mx.plot(color='blue', edgecolor='y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to examining the attribute values it is important to note that the\n",
    "spatial units for these states are far from homogenous in their shapes and\n",
    "sizes. This can have major impacts on our brain's pattern recognition capabilities\n",
    "as we tend to be drawn to the larger polygons. Yet, when we considered the\n",
    "statistical distribution above, each observation was given equal weight. Thus\n",
    "the spatial distribution becomes more complicated to evaluate from a visual and\n",
    "statistical perspective.\n",
    "\n",
    "With this qualification in mind, we will explore the construction of choropleth\n",
    "maps using PySAL and the helper method `choropleth`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choropleth(mx, 'PCGDP1940', cmap='BuGn', edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default uses a quantile classification with k=5, together with a green color\n",
    "pallete where darker hues indicate higher class assignment for the attribute\n",
    "values associated with each polygon.\n",
    "\n",
    "These defaults can be overriden in a\n",
    "number of ways, for example changing the colormap and number of quantiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choropleth(mx, 'PCGDP1940', cmap='Blues', k=4, edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will continue to use the `Blues`\n",
    "colormap in what follows in order to examine the spatial distribution revealed by\n",
    "each of the k=5 classifiers:\n",
    "\n",
    "- Equal Interval\n",
    "- Quantiles\n",
    "- Mean-Standard deviation\n",
    "- Maximum breaks\n",
    "- Box-Plot\n",
    "- Head Tail\n",
    "- Jenks Caspall\n",
    "- Fisher Jenks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choropleth(mx, 'PCGDP1940', cmap='Blues',\n",
    "           scheme='equal_interval', edgecolor='black', k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choropleth(mx, 'PCGDP1940', cmap='Blues',\n",
    "           scheme='maximum_breaks', edgecolor='black', k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choropleth(mx, 'PCGDP1940', cmap='Blues',\n",
    "#           scheme='Box_Plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choropleth(mx, 'PCGDP1940', cmap='Blues', \n",
    "#           scheme='headtail_breaks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.random.seed(12345)\n",
    "#choropleth(mx, 'PCGDP1940', cmap='Blues',\n",
    "#           scheme='jenks_caspall', k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.random.seed(12345)\n",
    "choropleth(mx, 'PCGDP1940', cmap='Blues', \n",
    "           scheme='fisher_jenks', edgecolor='black', k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diverging Attributes\n",
    "\n",
    "A slightly different type of attribute is the so-called \"diverging\" values attribute. This is\n",
    "useful when one wishes to place equal emphasis on mid-range critical values as\n",
    "well as extremes at both ends of the distribution. Light colors are used to\n",
    "emphasize the mid-range class while dark colors with contrasting hues are used\n",
    "to distinguish the low and high extremes.\n",
    "\n",
    "To illustrate this for the Mexican\n",
    "income data we can derive a new variable which measures the change in a state's\n",
    "rank in the income distribution between 1940 to 2000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnk = mx.rank(ascending=False) # ascending ranks 1=high, n=lowest\n",
    "rnk['NAME']=mx['NAME']\n",
    "delta_rnk = rnk.PCGDP1940 - rnk.PCGDP2000\n",
    "delta_rnk\n",
    "cls = numpy.digitize(delta_rnk, [-5, 0, 5, 20])\n",
    "cls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have created four classes for the rank changes: [-inf, -5), [-5, 0), [0,\n",
    "5), [5, 20]. Note that these are descending ranks, so the wealthiest state in\n",
    "any period has a rank of 1 and therefore when considering the change in ranks, a\n",
    "negative change reflects moving down the income distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choropleth(mx.assign(rkd=cls), 'rkd', cmap='RdYlBu',\n",
    "           scheme='equal_interval', k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the red (blue) hues are states that have moved downwards (upwards) in the\n",
    "income distribution, with the darker hue representing a larger movement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative Attributes\n",
    "\n",
    "The Mexico data set also has several variables that\n",
    "are on a nominal measurement scale. One of these is a region definition variable\n",
    "that groups individual states in contiguous clusters of similar characteristics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx['HANSON98'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This regionalization scheme partions Mexico into 5 regions. A naive (and\n",
    "incorrect) way to display this would be to treat the region variable as\n",
    "sequential and use equal_inteval $k=5$ to display the regions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choropleth(mx, 'HANSON98', cmap='Blues', scheme='equal_interval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not correct because the region variable is not on an interval scale, so\n",
    "the differences between the values have no quantitative significance but rather\n",
    "the values simply indicate region membership. However, the choropleth above gives\n",
    "a clear visual cue that regions in the south are more (of whichever is being plot)\n",
    "than those in the north, as the color map implies an intensity gradient.\n",
    "\n",
    "A more appropriate visualization\n",
    "is to use a \"qualitative\" color palette:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choropleth(mx, 'HANSON98', cmap='Pastel1',\n",
    "           scheme='equal_interval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.plot(column='HANSON98', categorical=True, cmap='Pastel1', legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this chapter we have considered the construction of choropleth maps for\n",
    "spatial data visualization. The key issues of the choice of classification\n",
    "scheme, variable measurement scale, spatial configuration and color palettes\n",
    "were illustrated using PySAL's map classification module together with other\n",
    "related packages in the PyData stack.\n",
    "\n",
    "Choropleth maps are a central tool in the geographic data science arsenal as\n",
    "they provide powerful visualizations of the spatial distribution of attribute\n",
    "values. We have only touched on the basic concepts in this chapter, as there is\n",
    "much more that can be said about cartographic theory and the design of effective\n",
    "choropleth maps. Readers interested in pursuing this literature are encouraged\n",
    "to see the references cited.\n",
    "\n",
    "At the same time, given the philosophy underlying PySAL the methods we cover\n",
    "here are sufficient for exploratory data analysis where rapid and flexible\n",
    "generation of views is critical to the work flow. Once the analysis is complete\n",
    "and the final presentation quality maps are to be generated, there are excellent\n",
    "packages in the data stack that the user can turn to.\n",
    "\n",
    "\n",
    "\n",
    "## References\n",
    "\n",
    "Duque, J.C., L. Anselin, and S.J. Rey. (2012) \"The max-p regions problem.\" *Journal of Regional Science*, 52:397-419.\n",
    "\n",
    "Jenks, G. F., & Caspall, F. C. (1971). Error on choroplethic maps: definition, measurement, reduction. Annals of the Association of American Geographers, 61(2), 217-244.\n",
    "\n",
    "Jian, B. (2013) \"Head/Tail Breaks: A New Classification Scheme for Data with a Heavy-Tailed Distribution.\" *The Professional Geographer*, 65(3): 482-494.\n",
    "\n",
    "Jiang, Bin. (2015) \"Head/tail breaks for visualization of city\n",
    "structure and dynamics.\" *Cities*, 43: 69-77.\n",
    "\n",
    "Rey, S.J. and M.L. Guitierez. (2010)\n",
    "\"Interregional inequality dynamics in Mexico.\" *Spatial Economic Analysis*, 5:\n",
    "277-298."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
